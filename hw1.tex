\documentclass[12pt,letterpaper, onecolumn]{exam}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{nicefrac}
\setcounter{MaxMatrixCols}{20}
\usepackage[lmargin=71pt, tmargin=1.2in]{geometry}  %For centering solution box
\lhead{Optimal Estimation}
\rhead{Noah Miller}
\thispagestyle{empty}   %For removing header/footer from page 1

\begin{document}

\begingroup
\centering
\LARGE Optimal Estimation\\
\LARGE Homework 1 \\[0.5em]
\large \today\\[0.5em]
\large Noah Miller\par
\large 903949330\par
\large MECH 7710\par
\endgroup
\pointsdroppedatright   %Self-explanatory
\printanswers
\renewcommand{\solution}{\noindent\textbf{Ans:}\enspace}   %Replace "Ans:" with starting keyword in solution box



\begin{questions}
	\question{Use the MATLAB function $>>conv$ to produce discrete probability functions\\
		(PDF’s) for throws of six dice as follows (note: this is effectively the sum of 6 random variables)}
	\begin{parts}
		\part{6 numbered: $1,\,2\,,3\,,4,\,5,\,6$}\\
		\solution
		We can write the probability density function of the first dice as equal to
		\[
			\begin{bmatrix}
				0 & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} \\
			\end{bmatrix} \]
		where the first index $0$ has no probability of occurring because the dice cannot roll a 0. Plotting the convolutions for each dice is displayed below along with the mean and standard deviations for each case in the title. The mean and standard deviations for case were found using equations~\ref{eq:1} and~\ref{eq:2}.
		\begin{equation}\label{eq:1}
			\overline{x} = \sum x \cdot f_x^T
		\end{equation}
		\begin{equation}\label{eq:2}
			\sigma_x = \sqrt{(x - \overline{x})^2 \cdot f_x^T}
		\end{equation}
		\begin{figure}[!h]
			\centering
			\includegraphics[width=.91\linewidth]{Q1_a.png}
		\end{figure}
		\clearpage
		\part{6 numbered: $4,\,5\,,6\,,7,\,8,\,9$}\\
		\solution
		\begin{figure}[!h]
			\centering
			\includegraphics[width=.91\linewidth]{Q1_b.png}
		\end{figure}
		\part{6 numbered: $1,\,1\,,3\,,3,\,3,\,5$}\\
		\solution
		\begin{figure}[!h]
			\centering
			\includegraphics[width=.91\linewidth]{Q1_c.png}
		\end{figure}
		\clearpage
		\part{6 numbered: $1,\,2\,,3\,,4,\,5,\,6$ and 3 numbered: $1,\,1\,,3\,,3,\,3,\,5$}\\
		\solution
		\begin{figure}[!h]
			\centering
			\includegraphics[width=.91\linewidth]{Q1_d.png}
		\end{figure}
	\end{parts}
	Check that the $\sum$PDF $= 1.0$

	Plot each PDF with a normal distribution plot of the same average and sigma.

	\textit{Note that even peculiar random distributions, taken in aggregate, tend to produce
		“normal” error distributions}
	\clearpage
	\question{What is the joint PDF for 2 fair dice $(\mathbf{X}_1, \mathbf{X}_2)$ (make this a $6\times6$ matrix with the indices equal to
		the values of the random variables). Note each row should add to the probability of the index
		for $x_1$ and each column to the probability of the index for $x_2$}\\
	\solution
	We define the rolls of two fair dice in a $6 \times 6$ matrix as follows:
	\begin{equation}
		PDF(\mathbf{X}_1, \mathbf{X}_2) =
		\frac{1}{36}
		\begin{bmatrix}
			1 & 1 & 1 & 1 & 1 & 1 \\
			1 & 1 & 1 & 1 & 1 & 1 \\
			1 & 1 & 1 & 1 & 1 & 1 \\
			1 & 1 & 1 & 1 & 1 & 1 \\
			1 & 1 & 1 & 1 & 1 & 1 \\
			1 & 1 & 1 & 1 & 1 & 1 \\
		\end{bmatrix}
	\end{equation}
	where the dice have the following numbered faces:
	\[\textbf{Dice 1:}
		\begin{bmatrix}
			1 & 2 & 3 & 4 & 5 & 6 \\
		\end{bmatrix} \]
	\[\textbf{Dice 2:}
		\begin{bmatrix}
			1 & 2 & 3 & 4 & 5 & 6 \\
		\end{bmatrix} \]
	\begin{parts}
		\part{What are
			\begin{subparts}
				\subpart{$E(\mathbf{X}_1)$}\\
				\solution
				\begin{equation}
					\begin{split}
						E(\mathbf{X}_1) & =\sum_{k = 1}^{6}\left(\mathbf{X}_1 f_{x_1}\left(\mathbf{x}_1 \right) \right)\\
						E(\mathbf{X}_1) & =
						\begin{bmatrix}
							1 & 2 & 3 & 4 & 5 & 6 \\
						\end{bmatrix}
						\begin{bmatrix}
							1/6 \\
							1/6 \\
							1/6 \\
							1/6 \\
							1/6 \\
							1/6 \\
						\end{bmatrix}\\
						E(\mathbf{X}_1) & = 3.5\\
					\end{split}
				\end{equation}
				\subpart{$E(\mathbf{X}_1 - E(\mathbf{X}_1))$}\\
				\solution
				\begin{equation}
					\begin{split}
						E(\mathbf{X}_1 - E(\mathbf{X}_1)) & = E(\mathbf{X}_1 - 3.5)\\
						E(\mathbf{X}_1 - E(\mathbf{X}_1)) & = E(\mathbf{X}_1) - 3.5\\
						E(\mathbf{X}_1 - E(\mathbf{X}_1)) & = 3.5 - 3.5\\
						E(\mathbf{X}_1 - E(\mathbf{X}_1)) & = 0\\
					\end{split}
				\end{equation}
				\clearpage
				\subpart{$E(\mathbf{X}_1^2)$}\\
				\solution
				\begin{equation}
					\begin{split}
						E(\mathbf{X}_1^2) & =
						\begin{bmatrix}
							1^2 & 2^2 & 3^2 & 4^2 & 5^2 & 6^2 \\
						\end{bmatrix}
						\begin{bmatrix}
							1/6 \\
							1/6 \\
							1/6 \\
							1/6 \\
							1/6 \\
							1/6 \\
						\end{bmatrix}\\
						E(\mathbf{X}_1^2) & =
						\begin{bmatrix}
							1 & 4 & 9 & 16 & 25 & 36 \\
						\end{bmatrix}
						\begin{bmatrix}
							1/6 \\
							1/6 \\
							1/6 \\
							1/6 \\
							1/6 \\
							1/6 \\
						\end{bmatrix}\\
						E(\mathbf{X}_1^2) & = 15.17\\
					\end{split}
				\end{equation}
				\subpart{$E((\mathbf{X}_1 - E(\mathbf{X_1}))^2)$}\\
				\solution
				\begin{equation}
					\begin{split}
						E\left((\mathbf{X}_1 - E(\mathbf{X}_1))^2\right) & = E\left((\mathbf{X}_1 - 3.5)^2\right)\\
						E\left((\mathbf{X}_1 - E(\mathbf{X}_1))^2\right) & =
						\begin{bmatrix}
							\frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} & \frac{1}{6} \\
						\end{bmatrix}
						\left(
						\begin{bmatrix}
								1 \\
								2 \\
								3 \\
								4 \\
								5 \\
								6 \\
							\end{bmatrix} -
						\begin{bmatrix}
								3.5 \\
								3.5 \\
								3.5 \\
								3.5 \\
								3.5 \\
								3.5 \\
							\end{bmatrix}
						\right)^2\\
						E\left((\mathbf{X}_1 - E(\mathbf{X}_1))^2\right) & = 2.92\\
					\end{split}
				\end{equation}
				\subpart{$E(((\mathbf{X}_1 - E(\mathbf{X}_1))\cdot(\mathbf{X}_2 - E(\mathbf{X}_2)))$}\\
				\solution
				{Because the rolls of two different, but equal and fair dice are independent events, we can say the expectation of their combined rolls are uncorrelated and thusly 0.}
				\begin{equation}
					\begin{split}
						E(((\mathbf{X}_1 - E(\mathbf{X}_1))\cdot(\mathbf{X}_2 - E(\mathbf{X}_2))) & = 0\\
					\end{split}
				\end{equation}

			\end{subparts}}
		\part{Form the covariance matrix for $x_1$ and $x_2$}\\
		\solution
		We can find the diagonal indices of the $x_1$,$x_2$ covariance matrix following a similar fashion that was done in class.
		\begin{equation}\begin{split}\label{eq:9}
				P_{11} & = E(x_1^2) - E(x_1)^2\\
				P_{11} & = 15.17 - 3.5^2\\
				P_{11} & = 2.917\\
			\end{split}\end{equation}
		\begin{equation}\begin{split}
				P_{22} & = E(x_2^2) - E(x_2)^2\\
				P_{22} & = 15.17 - 3.5^2\\
				P_{22} & = 2.917\\
			\end{split}\end{equation}
		As we stated earlier, $x_1$ and $x_2$ are independent and uncorrelated, so the off-diagonal indices of the covariance matrix are simply 0, Giving us
		\[\mathbf{P} =
			\begin{bmatrix}
				2.917 & 0     \\
				0     & 2.917 \\
			\end{bmatrix}.\]

		\part{Now find the PDF matrix for the variables $v_1 = x_1$ and $v_2 = x_1 + x_2$.}\\
		\solution
		\begin{equation}
			\begin{split}
				PDF(v_1,v_x) & = \frac{1}{36}\cdot
				\begin{bmatrix}
					1 & 1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 \\
					0 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\
					0 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 \\
					0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 0 \\
					0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 0 \\
					0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 1 \\
				\end{bmatrix}\\
			\end{split}
		\end{equation}
		\part{Now what is the mean, $E(v_1 - E(v_1))$, rms, and variance of $v_1$}\\
		\solution
		\begin{subparts}
			\subpart{The mean of $v_1$ is just the mean of $x_1$, which we already determined to be $3.5$.}
			\subpart
			{\begin{equation}
					E(v_1 - E(v_1)) = E(x_1 - E(x_1)) = 0
				\end{equation}}
			\subpart{The root mean square value for $v_1$ is just the square root of $E(x_1^2)$, or $3.89$.}
			\subpart{The variance of $v_1$ is just equation~\ref{eq:9} and is equal to $2.917$.}
		\end{subparts}

		\part{what is the mean, $E(v_2 - E(v_2))$, rms, and variance of $v_2$}\\
		\solution

		The mean of $v_2$ is just double the mean of $x_1$ since $v_2$ is the sum of $x_1$ and $x_2$. This gives $v_2$ a mean of 7.

		\begin{equation}
			E(v_2 - E(v_2)) = 2\cdot E(x_1 - E(x_1)) = 0
		\end{equation}

		The root mean square value for $v_2$ is just double the square root of $E(x_1^2)$, or $7.41$.

		The variance of $v_1$ is just double equation~\ref{eq:9} and is equal to $5.83$.
		\part{What is the new covariance matrix $P$.}\\
		\solution
		Using parts \textit{d} and \textit{e}, we can create a new covariance matrix, $\mathbf{P}$.
		\begin{equation}
			\mathbf{P} =
			\begin{bmatrix}
				2.917  & P_{21} \\
				P_{12} & 5.83   \\
			\end{bmatrix}
		\end{equation}
		In order to solve for the off-diagonals of the co-variance matrix we can solve
		\[E\left[(v_1 - \overline{v_1})(v_2 - \overline{v_2}) \right] \]
		Filling out what we already know simplifies this problem greatly.
		\begin{equation}
			\begin{split}
				E\left[(v_1 - \overline{v_1})(v_2 - \overline{v_2}) \right] & = E\left[(v_1 - 3.5)(v_2 - 7) \right]\\
				E\left[(v_1 - \overline{v_1})(v_2 - \overline{v_2}) \right] & = E\left[v_1v_2 - 7v_1 - 3.5v_2 + 24.5\right]\\
				v_2 & = x_1 + x_2\\
				v_1 & = x_1\\
				E\left[(v_1 - \overline{v_1})(v_2 - \overline{v_2}) \right] & = E\left[x_1^2 + x_1x_2 - 7x_1 - 3.5(x_1 + x_2) + 24.5\right]\\
				E\left[(v_1 - \overline{v_1})(v_2 - \overline{v_2}) \right] & = 15.17 + 12.25 - 24.5 - 12.25 - 12.25 + 24.5\\
				E\left[(v_1 - \overline{v_1})(v_2 - \overline{v_2}) \right] & = 2.92\\
			\end{split}
		\end{equation}
		This gives us a covariance matrix of:
		\begin{equation}
			\mathbf{P} =
			\begin{bmatrix}
				2.917 & 2.92 \\
				2.92  & 5.83 \\
			\end{bmatrix}
		\end{equation}
	\end{parts}
	\clearpage
	\question{Two random vectors $\mathbf{X}_1$ and $\mathbf{X_2}$ are uncorrelated if \[E\{(\mathbf{X}_1 - \overline{\mathbf{X}}_1)(\mathbf{X}_2 - \overline{\mathbf{X}}_2) \} = 0 \]} Show that:
	\begin{parts}
		\part{Independent random vectors are uncorrelated}\\
		\solution
		In order to show that independent random vectors are uncorrelated, we must prove
		\[>>cov(\mathbf{X}_1,\mathbf{X}_2)\triangleq E\{(\mathbf{X}_1 - \overline{\mathbf{X}}_1)(\mathbf{X}_2 - \overline{\mathbf{X}}_2)\} = 0\]
		to be true. We start by expanding terms.
		\begin{equation}
			\begin{split}
				>>cov(\mathbf{X}_1,\mathbf{X}_2) & \triangleq E\{\mathbf{X}_1\mathbf{X}_2 - \mathbf{X}_1\overline{\mathbf{X}}_2 - \mathbf{X}_1\overline{\mathbf{X}}_1 + \overline{\mathbf{X}}_1\overline{\mathbf{X}}_2\}\\
				0 & = E\{\mathbf{X}_1\mathbf{X}_2\} - E\{\mathbf{X}_1\overline{\mathbf{X}}_2\} - E\{\mathbf{X}_2\overline{\mathbf{X}}_1\} + E\{\overline{\mathbf{X}}_1\overline{\mathbf{X}}_2\}\\
			\end{split}
		\end{equation}
		With the terms expanded, we can simplify each group and then combine like terms.
		\begin{equation}
			\begin{split}
				E\{\mathbf{X}_1\mathbf{X}_2\} & = E\{\mathbf{X}_1\}E\{\mathbf{X}_2\}\; = \;\overline{\mathbf{X}}_1\overline{\mathbf{X}}_2\\
				-E\{\mathbf{X}_1\overline{\mathbf{X}}_2\} & = -\overline{\mathbf{X}}_2E\{\mathbf{X}_1\} \; = \;-\overline{\mathbf{X}}_2\overline{\mathbf{X}}_1 \\
				-E\{\mathbf{X}_2\overline{\mathbf{X}}_1\} & = -\overline{\mathbf{X}}_1E\{\mathbf{X}_2\} \; = \;-\overline{\mathbf{X}}_1\overline{\mathbf{X}}_2 \\
				E\{\overline{\mathbf{X}}_1\overline{\mathbf{X}}_2\} & = \overline{\mathbf{X}}_1\overline{\mathbf{X}}_2\\
			\end{split}
		\end{equation}
		\begin{equation}
			\begin{split}
				0 & = \overline{\mathbf{X}}_1\overline{\mathbf{X}}_2 -\overline{\mathbf{X}}_2\overline{\mathbf{X}}_1 -\overline{\mathbf{X}}_1\overline{\mathbf{X}}_2 + \overline{\mathbf{X}}_1\overline{\mathbf{X}}_2\\
				0 & = 0\\
			\end{split}
		\end{equation}

		\part{Uncorrelated Gaussian random vectors are independent}
		\solution
		We know from class that a joint Gaussian distribution can be described by the following function (equation~\ref{eq:20}).
		\begin{equation}\label{eq:20}
			f_{x,y}(x,y) = \frac{exp\left(\frac{-1}{2(1 - \rho_{12}^2)}\left(\frac{x^2}{\sigma_1^2} - 2\rho_{12}\frac{xy}{\sigma_1\sigma_2}+\frac{y^2}{\sigma^2_2}\right) \right)}{2\pi\sigma_1\sigma_2\sqrt{1 - \rho_{12}^2}}
		\end{equation}
		Uncorrelated Gaussian vectors have $\rho_{12} = 0$, and a substitution narrows down this function.
		\begin{equation}
			\begin{split}
				f_{x,y}(x,y) & = \frac{exp\left(\frac{-1}{2(1 - (0)^2)}\left(\frac{x^2}{\sigma_1^2} - 2(0)\frac{xy}{\sigma_1\sigma_2}+\frac{y^2}{\sigma^2_2}\right) \right)}{2\pi\sigma_1\sigma_2\sqrt{1 - (0)^2}}\\
				& = \frac{exp\left(\frac{-1}{2}\left(\frac{x^2}{\sigma_1^2}+\frac{y^2}{\sigma^2_2}\right) \right)}{2\pi\sigma_1\sigma_2}\\
			\end{split}
		\end{equation}
		Expanding the exponential in the numerator of the function allows us to match the definition of independence from our notes.
		\begin{equation}
			\begin{split}
				f_{x,y}(x,y) & = \frac{exp\left(\frac{-x^2}{2\sigma_1^2}\right)exp\left(\frac{-y^2}{2\sigma_1^2}\right)}{2\pi\sigma_1\sigma_2}\\
				f_{x}(x)f_{y}(y) & = \frac{exp\left(\frac{-x^2}{2\sigma_1^2}\right)}{2\pi\sigma_1}\frac{exp\left(\frac{-y^2}{2\sigma_1^2}\right)}{\sigma_2}\\
			\end{split}
		\end{equation}
	\end{parts}
	\clearpage
	\question{Consider a sequence created by throwing a pair of dice and summing the numbers which are $\{-2.5,-1.5,-0.5,0.5,1.5,2.5\}$. Call this $V_o(k)$.}
	\begin{parts}
		\part{What is the PDF?}
		\part{What are the mean and variance of this sequence?}\\
		\solution
		{The mean and standard deviation of the sequence can be found with the PDF in figure~\ref{fig:Q4_A}.
			\begin{figure}[!h]
				\label{fig:Q4_A}
				\centering
				\includegraphics[width=\linewidth]{Q4_a.png}
			\end{figure}}

		If we generate a new random sequence \[V_N(k+1) = (1 - r)V_N(k) + rV_o(k)\]where $V_N(k)$ is serially-correlated (not white).
		\part{In steady state, what are the mean and variance of this new sequence $(V_N)$}\\
		\solution
		{If we lay out a few iterations of the sequence for increasing values of $k$ we can find the mean and variance.
			\begin{equation}
				\begin{split}
					V_N(0) & = 0\\
					k_0: \quad V_N(1) & = (1-r)V_N(0) + rV_o(0)\\
					& = rV_o(0)\\
					k_1: \quad V_N(2) & = (1-r)V_N(1) + rV_o(1)\\
					& = (1-r)rV_o(0) + rV_o(1)\\
					k_2: \quad V_N(3) & = (1-r)V_N(2) + rV_o(2)\\
					& = (1-r)(1-r)rV_o(0) + rV_o(1) + rV_o(2)\\
					& = (1-r)^2rV_o(0) + r(V_o(1) + V_o(2))\\
					\vdots \qquad&\qquad \vdots\\
					k_j: \quad V_N(j) & = (1-r)^{j-1}V_N(j-1) + rV_o(j-1)\\
				\end{split}
			\end{equation}
			Seeing that $V_o$ has a near-zero mean, and that $V_N(k)$ depends on $V_o$, we can confidently say that the expectation of $V_N(k)$ is also near-zero. To find the variance, we can run a Monte-Carlo simulation.
			\begin{figure}[!h]
				\centering
				\includegraphics[width=\linewidth]{Q4_c.png}
			\end{figure}
		}
		\part{What is the covariance function: $R(k) = E\{V_N(k)V_N(k-L)\}$}

		(Hint: $V_N(k)$ and $V_o(k)$ are uncorrelated).

		\solution
		{We can start by expanding on the quotients inside of the covariance function. Instead of $k$ for indices of the sequences, I am using $j$.
			\begin{equation}
				\begin{split}
					V_N(j)V_N(j-L) & = (1-r)^jV_o(0) + r(1-r)^{j-1}V_o(1) + r(1-r)^{j-2}V_o(2)\\
					& + r(1-r)V_o(j-1) + rV_o(j)(1-r)^{j-L}V_o(0)\\
					& + r(1-r)^{j-L-1}V_o(1) + r(1-r)^{j-L-2}V_o(2)\\
					& + r(1-r)V_o(n-L-1) + rV_o(n-L)\\
				\end{split}
			\end{equation}
			Taking out the uncorrelated terms with an expectation equal to zero, reduces this long general form down to
			\begin{equation}
				\begin{split}
					V_N(j)V_N(j-L) & = (1-r)^{2j-L}V_o(0)^2 + r^2(1-r)^{2j-L-1}V_o(1)^2\\
					& + r^2(1-r)^{2n-2L}V_o(n-L)^2\\
				\end{split}
			\end{equation}
			Seeing as the first term is just a constant, and the term $V_o(n)^2$ is just the variance of the system, we can rewrite the function as
			\begin{equation}
				\begin{split}
					E\left[V_N(j)V_N(j-L)\right] & = (1-r)^{2j-L}\sigma_o^2 + r^2(1-r)^{2j-L-1}\sigma_o^2\\
					& + \dotsc + r^2(1-r)^{2n-2L}\sigma_o^2\\
				\end{split}
			\end{equation}
		}
		\part{Are there any practical constraints on $r$?}\\
		\solution{The constraints on r depend on the qoutient $(1-r)$. Because of this $r$ must be less than $1$ but greater than 0, otherwise, the system will grow unbounded.}
	\end{parts}
	\clearpage
	\question{A random variable $x$ has a PDF given by:
		\[
			f_\mathbf{x}(x)=
			\begin{cases}
				0           & \text{if } x < 0           \\
				\frac{x}{2} & \text{if } 0 \leq x \leq 2 \\
				0           & \text{if } x \geq 2        \\
			\end{cases}
		\]}
	\begin{parts}
		\part{What is the mean of x?}
		\solution{
			We know the general form for a continuous function to be
			\[\overline{x} = \int^{\infty}_{-\infty}xf(x)dx \]
			Because of the piecewise function we can use this general form for each interval.
			\begin{equation}
				\begin{split}
					\overline{x} & = \int^{0}_{-\infty}x(0)dx + \int^{2}_{0}x(\frac{x}{2})dx + \int^{\infty}_{2}x(0)dx\\
				\end{split}
			\end{equation}
			The first and last term result to 0,leaving us with the middle term. This leaves us with
			\begin{equation}
				\begin{split}
					\overline{x} & = \int^{2}_{0}x(\frac{x}{2})dx\\
					\overline{x} & = \frac{4}{3}\\
				\end{split}
			\end{equation}
		}
		\part{what is the variance of x?}\\
		\solution{
			We know the general form of variance to be
			\[var(x) = \sigma^2 = E\left[x^2 - \overline{x}^2 \right] \]
			Changing this to fit our particular form, and disregarding the first and second intervals of the piecewise function gives us
			\begin{equation}
				\begin{split}
					E\left[x^2 - \overline{x}^2 \right] & = \int^2_0 x^2\left(\frac{x}{2}\right)dx - \frac{16}{9}\\
					E\left[x^2 - \overline{x}^2 \right] & = 2 - 1.777778\\
					E\left[x^2 - \overline{x}^2 \right] & = 0.22222\\
				\end{split}
			\end{equation}
		}
	\end{parts}
	\clearpage
	\question{Consider a normally distributed two-dimensional vector $\mathbf{X}$, with zero mean and
		\[P_\mathbf{X} =
			\begin{bmatrix}
				2 & 1 \\
				1 & 4 \\
			\end{bmatrix} \]
	}
	\begin{parts}
		\part{Find the eigenvalues of $P_{\mathbf{X}}$}\\
		\solution
		{Using $>>eig$ in MATLAB easily produces the eigenvalues of $P_{\mathbf{X}}$
			\begin{equation}
				\begin{split}
					>>eig(P_{\mathbf{X}}) & = [1.5858\quad4.4142]\\
				\end{split}
			\end{equation}
		}
		\part{The likelihood ellipses are given by an equation of the form: $x^T P^{-1}_x x = c^2$. What are the principal axes in this case?}\\
		\solution{
			While we could use the likelihood ellipse equation to determine the principal axes, it is also known that the principles axes are just the eigenvectors of $\mathbf{P}_x$. We can use $>>eig$ once again to find these axes.
			\begin{equation}
				\begin{split}
					>>eig(\mathbf{P}_x) & =
					\begin{bmatrix}
						-0.92388 & 0.38268 \\
						0.38268  & 0.92388 \\
					\end{bmatrix}\\
				\end{split}
			\end{equation}}
		\part{Plot the likelihood ellipses for $c = 0.25$, $1$, $1.5$}\\
		\solution{
			Using the equation of ellipse we are able to plot the likelihood at each value for $c$.
			\begin{figure}[!h]
				\centering
				\includegraphics[width=0.85\linewidth]{Q6_c.png}
			\end{figure}
		}
		\clearpage
		\part{What is the probability of finding $\mathbf{X}$ inside each of these ellipses?}

		\solution{If we assume a linear relationship between the ellipses of\\ $c = [0.25\quad1\quad1.5]$, we can look at the \textit{ErrorEllipses.docx} that was provided to me, to interpolate between each of the values for $c$.

		According to the document, for $c = 1$ the probability of $\mathbf{X}$ lying inside the ellipse is $39.34\%$. For $c = .25$ and $1.5$, the probability is $9.835\%$ and $59.01\%$, respectively.
		}
	\end{parts}
	\clearpage
	\question{Given $x \sim N(0,\sigma_x^2)$ and $y = 2x^2$}
	\begin{parts}
		\part{Find the PDF of $y$}

		\solution{
			We know a general form of equation \ref{eq:32} exists when $g(x)$ is invertible and also continuously differentiable.
			\begin{equation}\label{eq:32}
				\begin{split}
					f_y(y) & = f_x(g^{-1}(y))\left|\frac{\partial g^{-1}(y)}{\partial y}
					\right|\\
				\end{split}
			\end{equation}
			From the problem statement, we can define $g(y)$ as $2x^2$ and $g^{-1}(y) = \sqrt{\frac{y}{2}}$. Differentiating $g^{-1}(x)$ leaves us with
			\[
				\left|\frac{\partial g^{-1}(y)}{\partial y}\right| = \frac{1}{2y\sqrt{2}}.
			\]
			Continuing on with our particular form of equation \ref{eq:32} gives us
			\begin{equation}
				\begin{split}
					f_y(y) & = f_y\left(\sqrt{\frac{y}{2}}\right)\frac{1}{2y\sqrt{2}}\\
					f_y(y) & = \frac{exp(-\frac{y}{4\sigma^2})}{\sqrt{2\pi\sigma^2}}\frac{1}{2y\sqrt{2}}\\
					f_y(y) & = \frac{exp(-\frac{y}{4\sigma^2})}{4y\sqrt{\pi\sigma^2}}\\
				\end{split}
			\end{equation}
		}
		\part{Draw the PDFs of $x$ and $y$ on the same plot for $\sigma_x = 2.0$}
		\solution{
			\begin{figure}[!h]
				\centering
				\includegraphics[width=0.81\linewidth]{Q7_b.png}
			\end{figure}
		}
		\clearpage

		\part{How has the density function changed by this transformation?}

		\solution
		{
			It seems that the density function has less variance, but still has zero-mean. It also has a greater spike about the mean.
		}
		\part{Is $y$ a normal random variable?}

		\solution{
			The variable $y$ is not a normal variable any more because of its inability to use 0. when $f_y(y=0)$, the function aproaches positive infinity.
		}
	\end{parts}
\end{questions}
\end{document}